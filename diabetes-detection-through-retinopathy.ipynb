{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2231974,"sourceType":"datasetVersion","datasetId":1340911}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/surajbansi28/diabetes-detection-through-retinopathy?scriptVersionId=226524863\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:04:37.333614Z","iopub.execute_input":"2025-03-08T11:04:37.333936Z","iopub.status.idle":"2025-03-08T11:05:27.198637Z","shell.execute_reply.started":"2025-03-08T11:04:37.33391Z","shell.execute_reply":"2025-03-08T11:05:27.19788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom typing import List, Tuple, Dict\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:32:35.30971Z","iopub.execute_input":"2025-03-08T12:32:35.310038Z","iopub.status.idle":"2025-03-08T12:32:40.950078Z","shell.execute_reply.started":"2025-03-08T12:32:35.310013Z","shell.execute_reply":"2025-03-08T12:32:40.94907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:32:42.394425Z","iopub.execute_input":"2025-03-08T12:32:42.394853Z","iopub.status.idle":"2025-03-08T12:32:42.398378Z","shell.execute_reply.started":"2025-03-08T12:32:42.394821Z","shell.execute_reply":"2025-03-08T12:32:42.397556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n# Set device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:32:44.522601Z","iopub.execute_input":"2025-03-08T12:32:44.523017Z","iopub.status.idle":"2025-03-08T12:32:44.598959Z","shell.execute_reply.started":"2025-03-08T12:32:44.522984Z","shell.execute_reply":"2025-03-08T12:32:44.598091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/train\"\ntest_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/test\"\nval_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/val\"\n\ndata_transform = transforms.Compose([\n    transforms.Resize(size=(224, 224)),\n    transforms.RandomHorizontalFlip(p=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor()\n])\n\n# Create ImageFolders for train, val and test datasets\ntrain_data = datasets.ImageFolder(root=train_dir,\n                                  transform=data_transform, # transform for the data\n                                  target_transform=None) # transform for the label/target\nval_data = datasets.ImageFolder(root=val_dir,\n                                transform=data_transform)\ntest_data = datasets.ImageFolder(root=test_dir,\n                                 transform=data_transform)\n\ntrain_data.classes, test_data.classes, val_data.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:32:46.904491Z","iopub.execute_input":"2025-03-08T12:32:46.904801Z","iopub.status.idle":"2025-03-08T12:33:32.395333Z","shell.execute_reply.started":"2025-03-08T12:32:46.904755Z","shell.execute_reply":"2025-03-08T12:33:32.394652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Define batch size\nbatch_size = 32  \n\n# Create data loaders\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:33:36.899637Z","iopub.execute_input":"2025-03-08T12:33:36.899988Z","iopub.status.idle":"2025-03-08T12:33:36.904446Z","shell.execute_reply.started":"2025-03-08T12:33:36.899959Z","shell.execute_reply":"2025-03-08T12:33:36.903739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load a pre-trained ResNet model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.resnet50(pretrained=True)\n\n# Modify the final layer to match our number of classes (5 for DR severity levels)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 5)  # 5 output classes (No_DR, Mild, Moderate, Severe, Proliferative_DR)\n\n# Move the model to GPU (if available)\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:33:41.285356Z","iopub.execute_input":"2025-03-08T12:33:41.285683Z","iopub.status.idle":"2025-03-08T12:33:42.627281Z","shell.execute_reply.started":"2025-03-08T12:33:41.285659Z","shell.execute_reply":"2025-03-08T12:33:42.626607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:33:43.90134Z","iopub.execute_input":"2025-03-08T12:33:43.901644Z","iopub.status.idle":"2025-03-08T12:33:43.907431Z","shell.execute_reply.started":"2025-03-08T12:33:43.901619Z","shell.execute_reply":"2025-03-08T12:33:43.906531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Training the Model.\nnum_epochs = 10  # Adjust based on performance\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()  # Clear previous gradients\n        outputs = model(images)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute loss\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n        \n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:07:00.608632Z","iopub.execute_input":"2025-03-08T11:07:00.608928Z","iopub.status.idle":"2025-03-08T11:39:58.466745Z","shell.execute_reply.started":"2025-03-08T11:07:00.608905Z","shell.execute_reply":"2025-03-08T11:39:58.465618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()  # Set model to evaluation mode\ncorrect, total = 0, 0\n\nwith torch.no_grad():  # No gradient updates in validation\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        \n        _, predicted = torch.max(outputs, 1)  # Get highest probability class\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f\"Validation Accuracy: {accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:33:46.157578Z","iopub.execute_input":"2025-03-08T12:33:46.157919Z","iopub.status.idle":"2025-03-08T12:34:33.646448Z","shell.execute_reply.started":"2025-03-08T12:33:46.157891Z","shell.execute_reply":"2025-03-08T12:34:33.64552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model\ntorch.save(model.state_dict(), \"diabetic_retinopathy_model.pth\")\nprint(\"Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:35:42.820238Z","iopub.execute_input":"2025-03-08T12:35:42.820577Z","iopub.status.idle":"2025-03-08T12:35:42.974667Z","shell.execute_reply.started":"2025-03-08T12:35:42.820554Z","shell.execute_reply":"2025-03-08T12:35:42.973751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\n\ntorch.cuda.empty_cache()\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Download dataset\nkushagratandon12_diabetic_retinopathy_balanced_path = kagglehub.dataset_download('kushagratandon12/diabetic-retinopathy-balanced')\n\ntrain_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/train\"\ntest_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/test\"\nval_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/val\"\n\n# Data transformation\ndata_transform = transforms.Compose([\n    transforms.Resize(size=(299, 299)),  # Adjusted for InceptionV3\n    transforms.RandomHorizontalFlip(p=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor()\n])\n\n# Load datasets\ntrain_data = datasets.ImageFolder(root=train_dir, transform=data_transform)\nval_data = datasets.ImageFolder(root=val_dir, transform=data_transform)\ntest_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T17:10:29.360586Z","iopub.execute_input":"2025-03-08T17:10:29.360938Z","iopub.status.idle":"2025-03-08T17:11:16.469182Z","shell.execute_reply.started":"2025-03-08T17:10:29.360912Z","shell.execute_reply":"2025-03-08T17:11:16.468005Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a4c93a208d60>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         samples = self.make_dataset(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         return make_dataset(\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mis_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# If is_dir() raises an OSError, consider that the entry is not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Load InceptionV3 model\nmodel = models.inception_v3(pretrained=True, aux_logits=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 5)  # 5 classes\nmodel = model.to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs.logits, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n# Save the trained model\nmodel_path = \"inceptionv3_diabetic_retinopathy.pt\"\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")\n\n# Validation\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f\"Validation Accuracy: {accuracy:.2f}%\")\n\n# Grad-CAM Implementation\ndef get_grad_cam(model, image, target_layer):\n    model.eval()\n    gradients = []\n    activations = []\n    \n    def save_gradient(grad):\n        gradients.append(grad)\n    \n    def forward_hook(module, input, output):\n        activations.append(output)\n        output.register_hook(save_gradient)\n    \n    target_layer.register_forward_hook(forward_hook)\n    \n    image = image.unsqueeze(0).to(device)\n    output = model(image)\n    class_idx = output.argmax(dim=1).item()\n    score = output[:, class_idx].squeeze()\n    \n    model.zero_grad()\n    score.backward()\n    \n    gradient = gradients[0].cpu().data.numpy()[0]\n    activation = activations[0].cpu().data.numpy()[0]\n    \n    weights = np.mean(gradient, axis=(1, 2))\n    cam = np.sum(weights[:, np.newaxis, np.newaxis] * activation, axis=0)\n    cam = np.maximum(cam, 0)\n    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))\n    \n    return cam\n\n# Example usage with test image\nsample_image, _ = test_data[0]\ntarget_layer = model.Mixed_7c\ncam = get_grad_cam(model, sample_image, target_layer)\n\nplt.imshow(sample_image.permute(1, 2, 0))\nplt.imshow(cam, cmap='jet', alpha=0.5)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T15:23:46.624006Z","iopub.execute_input":"2025-03-08T15:23:46.624349Z","iopub.status.idle":"2025-03-08T16:18:28.806769Z","shell.execute_reply.started":"2025-03-08T15:23:46.624324Z","shell.execute_reply":"2025-03-08T16:18:28.805887Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9687517478185541\nEpoch 2, Loss: 0.7467985416981665\nEpoch 3, Loss: 0.6046314702150138\nEpoch 4, Loss: 0.5117159968187266\nEpoch 5, Loss: 0.4382586518768221\nEpoch 6, Loss: 0.37056983901899965\nEpoch 7, Loss: 0.3161741023855832\nEpoch 8, Loss: 0.2693511610782212\nEpoch 9, Loss: 0.22992058602839655\nEpoch 10, Loss: 0.19950950499418574\nModel saved to inceptionv3_diabetic_retinopathy.pt\nValidation Accuracy: 82.17%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import kagglehub\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\n\ntorch.cuda.empty_cache()\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Download dataset\nkushagratandon12_diabetic_retinopathy_balanced_path = kagglehub.dataset_download('kushagratandon12/diabetic-retinopathy-balanced')\n\ntrain_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/train\"\ntest_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/test\"\nval_dir = \"/kaggle/input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/val\"\n\n# Data transformation\ndata_transform = transforms.Compose([\n    transforms.Resize(size=(299, 299)),  # Adjusted for InceptionV3\n    transforms.RandomHorizontalFlip(p=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor()\n])\n\n# Load datasets\ntrain_data = datasets.ImageFolder(root=train_dir, transform=data_transform)\nval_data = datasets.ImageFolder(root=val_dir, transform=data_transform)\ntest_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T17:15:37.556239Z","iopub.execute_input":"2025-03-08T17:15:37.556625Z","iopub.status.idle":"2025-03-08T17:16:18.311828Z","shell.execute_reply.started":"2025-03-08T17:15:37.556589Z","shell.execute_reply":"2025-03-08T17:16:18.310852Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nimport numpy as np\n\n# Load InceptionV3 model with pretrained weights\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the model with aux_logits=True first\nweights = models.Inception_V3_Weights.DEFAULT  # Get pretrained weights\nmodel = models.inception_v3(weights=weights, aux_logits=True)  # Must keep aux_logits=True to load weights\n\n# Disable aux_logits by removing the auxiliary classifier\nmodel.aux_logits = False  # Set to False\nmodel.AuxLogits = None  # Remove auxiliary classifier\n\n# Replace the final fully connected (fc) layer\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 5)  # 5 output classes\n\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n\n# Early stopping parameters\npatience = 5  # Stop if validation loss does not improve for 5 epochs\nbest_val_loss = np.inf\ncounter = 0\n\nnum_epochs = 50  # Increase as needed\n\n# Training loop with Early Stopping\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)  # No .logits needed\n        loss = criterion(outputs, labels)  \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    avg_train_loss = running_loss / len(train_loader)\n    \n    # Validation Phase\n    model.eval()\n    val_loss = 0.0\n    correct, total = 0, 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)  # No .logits needed\n            loss = criterion(outputs, labels)  \n            val_loss += loss.item()\n            \n            _, predicted = torch.max(outputs, 1)  # No .logits needed\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100 * correct / total\n\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n\n    # Early Stopping Check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        counter = 0\n        torch.save(model.state_dict(), \"model_best.pt\")  # Save the best model\n        print(\"✅ Model improved. Saved as model_best.pt\")\n    else:\n        counter += 1\n        print(f\"⏳ No improvement for {counter} epochs...\")\n\n        if counter >= patience:\n            print(\"⛔ Early stopping triggered! Training stopped.\")\n            break  # Stop training if no improvement\n\nprint(\"Training completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T17:50:29.134545Z","iopub.execute_input":"2025-03-08T17:50:29.13486Z","iopub.status.idle":"2025-03-08T19:39:06.502223Z","shell.execute_reply.started":"2025-03-08T17:50:29.134834Z","shell.execute_reply":"2025-03-08T19:39:06.501139Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Train Loss: 0.9684 | Val Loss: 0.7697 | Val Accuracy: 68.29%\n✅ Model improved. Saved as model_best.pt\nEpoch 2 | Train Loss: 0.7415 | Val Loss: 0.6744 | Val Accuracy: 70.99%\n✅ Model improved. Saved as model_best.pt\nEpoch 3 | Train Loss: 0.6076 | Val Loss: 0.5807 | Val Accuracy: 75.15%\n✅ Model improved. Saved as model_best.pt\nEpoch 4 | Train Loss: 0.5130 | Val Loss: 0.5369 | Val Accuracy: 76.48%\n✅ Model improved. Saved as model_best.pt\nEpoch 5 | Train Loss: 0.4362 | Val Loss: 0.5153 | Val Accuracy: 78.06%\n✅ Model improved. Saved as model_best.pt\nEpoch 6 | Train Loss: 0.3738 | Val Loss: 0.5232 | Val Accuracy: 78.83%\n⏳ No improvement for 1 epochs...\nEpoch 7 | Train Loss: 0.3111 | Val Loss: 0.5031 | Val Accuracy: 80.21%\n✅ Model improved. Saved as model_best.pt\nEpoch 8 | Train Loss: 0.2723 | Val Loss: 0.4907 | Val Accuracy: 81.26%\n✅ Model improved. Saved as model_best.pt\nEpoch 9 | Train Loss: 0.2305 | Val Loss: 0.4835 | Val Accuracy: 82.42%\n✅ Model improved. Saved as model_best.pt\nEpoch 10 | Train Loss: 0.2013 | Val Loss: 0.4920 | Val Accuracy: 82.97%\n⏳ No improvement for 1 epochs...\nEpoch 11 | Train Loss: 0.1742 | Val Loss: 0.4907 | Val Accuracy: 83.46%\n⏳ No improvement for 2 epochs...\nEpoch 12 | Train Loss: 0.1643 | Val Loss: 0.5069 | Val Accuracy: 83.86%\n⏳ No improvement for 3 epochs...\nEpoch 13 | Train Loss: 0.1498 | Val Loss: 0.4507 | Val Accuracy: 84.80%\n✅ Model improved. Saved as model_best.pt\nEpoch 14 | Train Loss: 0.1390 | Val Loss: 0.4590 | Val Accuracy: 84.65%\n⏳ No improvement for 1 epochs...\nEpoch 15 | Train Loss: 0.1309 | Val Loss: 0.4924 | Val Accuracy: 84.57%\n⏳ No improvement for 2 epochs...\nEpoch 16 | Train Loss: 0.1184 | Val Loss: 0.4690 | Val Accuracy: 85.78%\n⏳ No improvement for 3 epochs...\nEpoch 17 | Train Loss: 0.1151 | Val Loss: 0.4847 | Val Accuracy: 84.58%\n⏳ No improvement for 4 epochs...\nEpoch 18 | Train Loss: 0.1045 | Val Loss: 0.4620 | Val Accuracy: 85.37%\n⏳ No improvement for 5 epochs...\n⛔ Early stopping triggered! Training stopped.\nTraining completed!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import shutil\nshutil.make_archive('/kaggle/working/model', 'zip', '/kaggle/working', 'model_best.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:45:52.778066Z","iopub.execute_input":"2025-03-08T19:45:52.778434Z","iopub.status.idle":"2025-03-08T19:45:56.981708Z","shell.execute_reply.started":"2025-03-08T19:45:52.778411Z","shell.execute_reply":"2025-03-08T19:45:56.980957Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model.zip'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a downloadable link\nFileLink(\"/kaggle/working/model.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T20:02:16.943855Z","iopub.execute_input":"2025-03-08T20:02:16.944212Z","iopub.status.idle":"2025-03-08T20:02:16.949364Z","shell.execute_reply.started":"2025-03-08T20:02:16.944187Z","shell.execute_reply":"2025-03-08T20:02:16.948621Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='/kaggle/working/model.zip' target='_blank'>/kaggle/working/model.zip</a><br>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"!mv /kaggle/working/model.zip /kaggle/outputs/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T20:05:31.758888Z","iopub.execute_input":"2025-03-08T20:05:31.759281Z","iopub.status.idle":"2025-03-08T20:05:31.908228Z","shell.execute_reply.started":"2025-03-08T20:05:31.759251Z","shell.execute_reply":"2025-03-08T20:05:31.906973Z"}},"outputs":[{"name":"stdout","text":"mv: cannot create regular file '/kaggle/outputs/': Not a directory\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}